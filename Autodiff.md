# Autograd, Gradients, PyTorch source code

## sources:
- [Toronto Lecture Notes](https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/slides/lec10.pdf)
- [Medium Article](https://towardsdatascience.com/pytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95#:~:text=Backpropagation%20is%20used%20to%20calculate,and%20eventually%20reduce%20the%20loss.&text=Forward%20propagate%20on%20the%20architecture,the%20gradient%20for%20each%20weight)
